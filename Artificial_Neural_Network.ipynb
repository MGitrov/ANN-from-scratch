{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7182c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11d33a",
   "metadata": {},
   "source": [
    "# Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537a0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    A function that will apply the sigmoid activation function.\n",
    "    \n",
    "    Parameters:\n",
    "        - x (int, float): The sum of the dot product with the bias.\n",
    "        \n",
    "    Returns:\n",
    "        - float: The output of the neuron.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c829edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def feed_forward(self, inputs):\n",
    "        '''\n",
    "        A function that will perform the process of passing inputs forward to get an output.\n",
    "        \n",
    "        Parameters:\n",
    "            - inputs (int, float, list of ints / floats): Neuron's input.\n",
    "            \n",
    "        Returns:\n",
    "            - float: The output of the neuron.\n",
    "        '''\n",
    "        result = np.dot(a = self.weights, b = inputs) + self.bias\n",
    "        \n",
    "        return sigmoid(x = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d4a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_1 = 0, w_2 = 1\n",
    "weights = [0, 1]\n",
    "\n",
    "# b = 4\n",
    "bias = 4\n",
    "neuron = Neuron(weights = weights, bias = bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bce6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990889488055994"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_1 = 2, x_2 = 3\n",
    "inputs = [2, 3]\n",
    "neuron.feed_forward(inputs = inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd2db0",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f595f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    '''\n",
    "    A neural network in the form of:\n",
    "        - 2 input values\n",
    "        - 1 hidden layer with 2 neurons (h1, h2)\n",
    "        - An output layer with 1 neuron (o1)\n",
    "        \n",
    "    For simplicity, each neuron will have the same weights and bias as follows:\n",
    "        - weights = [0, 1]\n",
    "        - bias = 0\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        weights = [0, 1]\n",
    "        bias = 0\n",
    "        \n",
    "        self.h1 = Neuron(weights = weights, bias = bias)\n",
    "        self.h2 = Neuron(weights = weights, bias = bias)\n",
    "        self.o1 = Neuron(weights = weights, bias = bias)\n",
    "        \n",
    "    def feed_forward(self, inputs):\n",
    "        '''\n",
    "        A function that will perform the process of passing inputs forward to get an output.\n",
    "        \n",
    "        Parameters:\n",
    "            - inputs (list of ints / floats): Neural network's input.\n",
    "            \n",
    "        Returns:\n",
    "            - float: The output of the neural network.\n",
    "        '''\n",
    "        output_h1 = self.h1.feed_forward(inputs = inputs)\n",
    "        output_h2 = self.h2.feed_forward(inputs = inputs)\n",
    "        \n",
    "        # The inputs to the output layer are the outputs of the hidden layer.\n",
    "        output_o1 = self.o1.feed_forward(inputs = [output_h1, output_h2])\n",
    "        \n",
    "        return output_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8eed27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c98ab7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7216325609518421"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [2, 3]\n",
    "ann.feed_forward(inputs = inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a5bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    '''\n",
    "    A function that will calculate the loss in the form of mean squared error (MSE).\n",
    "    \n",
    "    Parameters:\n",
    "        - y_true (numpy array): The actual values of the target variable.\n",
    "        - y_pred (numpy array): The predicted values of the target variable.\n",
    "        \n",
    "    Returns:\n",
    "        - int / float: The loss.\n",
    "    '''\n",
    "    return ((y_pred - y_true) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538392d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "mse_loss(y_true = y_true, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea18bd",
   "metadata": {},
   "source": [
    "# Complete Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    '''\n",
    "    A function that will apply the derivative of the sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "        - x (int, float): The sum of the dot product with the bias.\n",
    "        \n",
    "    Returns:\n",
    "        - float.\n",
    "    '''\n",
    "    fx = sigmoid(x = x)\n",
    "    \n",
    "    return fx * (1 - fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dea873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C_Neural_Network():\n",
    "    '''\n",
    "    A neural network in the form of:\n",
    "        - 2 input values\n",
    "        - 1 hidden layer with 2 neurons (h1, h2)\n",
    "        - An output layer with 1 neuron (o1)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Weights - The neural network's weights are first set randomly.\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Biases - The neural network's biases are first set randomly.\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        '''\n",
    "        A function that will perform the process of passing inputs forward to get an output.\n",
    "        \n",
    "        Parameters:\n",
    "            - inputs (list of ints / floats): Neural network's input, which is the weight and height in our case, and thus\n",
    "            the input will a numpy array with 2 elements.\n",
    "\n",
    "        Returns:\n",
    "            - float: The output of the neural network.\n",
    "        '''\n",
    "        h1 = sigmoid(x = self.w1 * inputs[0] + self.w2 * inputs[1] + self.b1)\n",
    "        h2 = sigmoid(x = self.w3 * inputs[0] + self.w4 * inputs[1] + self.b2)\n",
    "        \n",
    "        # The inputs to the output layer are the outputs of the hidden layer.\n",
    "        o1 = sigmoid(x = self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "\n",
    "        return o1\n",
    "\n",
    "    def training(self, data, y_trues):\n",
    "        '''\n",
    "        abc\n",
    "        '''\n",
    "        learning_rate = 0.01\n",
    "        \n",
    "        # Iterations on the entire dataset, when one epoch means an entire dataset is passed forward and backward through the neural network \n",
    "        # only once. We are using a limited dataset and to optimize the weights we are using gradient descent which is an iterative process. \n",
    "        # So, updating the weights with a single epoch will not be enough.\n",
    "        epochs = 400\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, y_trues):\n",
    "                # For convenience, the sum of the dot product with the bias of h1, h2, and o1 will be calculated now, as we will have to use \n",
    "                # these values when the partial derivatives will be calculated.\n",
    "                sum_h1 = self.w1 * inputs[0] + self.w2 * inputs[1] + self.b1\n",
    "                h1 = sigmoid(x = sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * inputs[0] + self.w4 * inputs[1] + self.b2\n",
    "                h2 = sigmoid(x = sum_h2)\n",
    "        \n",
    "                # The inputs to the output layer are the outputs of the hidden layer.\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(x = sum_o1)\n",
    "                # o1 is assigned to y_pred, as he is the last neuron, and hence it will be the neural networkâ€™s prediction.\n",
    "                y_pred = o1\n",
    "\n",
    "                # Partial derivatives calculation, when dL_dw1 corresponds to the derivative of L with respect to w1.\n",
    "                dL_dypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # h1, h2 -> o1.\n",
    "                dypred_dw5 = h1 * sigmoid_derivative(x = sum_o1)\n",
    "                dypred_dw6 = h2 * sigmoid_derivative(x = sum_o1)\n",
    "                dypred_db3 = 1 * sigmoid_derivative(x = sum_o1)\n",
    "\n",
    "                dypred_dh1 = self.w5 * sigmoid_derivative(x = sum_o1)\n",
    "                dypred_dh2 = self.w6 * sigmoid_derivative(x = sum_o1)\n",
    "\n",
    "                # x1, x2 -> h1.\n",
    "                dh1_dw1 = inputs[0] * sigmoid_derivative(x = sum_h1)\n",
    "                dh1_dw2 = inputs[1] * sigmoid_derivative(x = sum_h1)\n",
    "                dh1_db1 = 1 * sigmoid_derivative(x = sum_h1)\n",
    "\n",
    "                # x1, x2 -> h2.\n",
    "                dh2_dw3 = inputs[0] * sigmoid_derivative(x = sum_h2)\n",
    "                dh2_dw4 = inputs[1] * sigmoid_derivative(x = sum_h2)\n",
    "                dh2_db2 = 1 * sigmoid_derivative(x = sum_h2)\n",
    "\n",
    "                # Weights and biases updation.\n",
    "                # h1 related - The parameters which affect only h1.\n",
    "                self.w1 -= learning_rate * dL_dypred * dypred_dh1 * dh1_dw1\n",
    "                self.w2 -= learning_rate * dL_dypred * dypred_dh1 * dh1_dw2\n",
    "                self.b1 -= learning_rate * dL_dypred * dypred_dh1 * dh1_db1\n",
    "\n",
    "                # h2 related - The parameters which affect only h2.\n",
    "                self.w3 -= learning_rate * dL_dypred * dypred_dh2 * dh2_dw3\n",
    "                self.w4 -= learning_rate * dL_dypred * dypred_dh2 * dh2_dw4\n",
    "                self.b2 -= learning_rate * dL_dypred * dypred_dh2 * dh2_db2\n",
    "\n",
    "                # o1 related - The parameters which affect only o1.\n",
    "                self.w5 -= learning_rate * dL_dypred * dypred_dw5\n",
    "                self.w6 -= learning_rate * dL_dypred * dypred_dw6\n",
    "                self.b3 -= learning_rate * dL_dypred * dypred_db3\n",
    "\n",
    "            y_preds = np.apply_along_axis(func1d = self.feed_forward, axis = 1, arr = data)\n",
    "            loss = mse_loss(y_true = y_trues, y_pred = y_preds)\n",
    "            print(\"Epoch %d/%d - loss: %.3f\" %(epoch, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(object = [   \n",
    "[-2, -1], # Alice\n",
    "[25, 6],  # Bob\n",
    "[17, 4],  # Charlie\n",
    "[-15, -6] # Diana\n",
    "])\n",
    "\n",
    "y_trues = np.array(object = [\n",
    "    1, # Alice\n",
    "    0, # Bob\n",
    "    0, # Charlie\n",
    "    1  # Diana\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a1711d0bf5b863f53104549f70b879233b46cc07d7206b040e92c4d761b405c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
